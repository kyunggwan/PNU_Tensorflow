{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNET\n",
    "- 층을 많이 쌓았을 때 성능이 안나오는 상황이 발생한다.\n",
    "- 층이 깊어질수록 맨 처음 들어왔던 정보가 끝까지 전달 안된다.\n",
    "- 가중치 소실도 일어날 수 있다.\n",
    "- 그러면 학습이 되지 않는다.\n",
    "\n",
    "- 그래서 층을 쌓으면서도 가중치 소실을 방지하고 싶어서 RESNET개발\n",
    "- 최적화가 멈추는 것을 방지하는 방법"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity block\n",
    "- 블록으로 데이터가 들어갔을 때, 블록에서 나온 출력에다가 맨 처음에 나온 데이터를 더해준다.\n",
    "- 다음층에서 학습해야할 양을 줄여준다.\n",
    "- 이전의 데이터를 받아옴으로써 학습양이 줄어듦\n",
    "- 이점 : 블록의 결과를 f(x)라는 함수라고 할 때, 그 f(x)를 x에 더해준다. \n",
    "    - 이것을 미분했을 때, x는 1로 되므로 가중치가 사라지지 않는다.\n",
    "    - gradient 소실은 0이 되므로 가중치가 소실 되는데, 이를 방지함으로 최적화가 멈추는 것을 방지한다.\n",
    "    -  그래서 층을 쌓을수록 데이터 손실을 방지한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution block\n",
    "- 위의 원리에서 x에 1*1 합성곱층을 한번 거쳐서 더해준다.\n",
    "- 채널의 수를 조절해준다(사실상 큰 의미는 없음)\n",
    "- 결론: 가중치 손실을 방지한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
