{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDoe7jazdPz5"
      },
      "outputs": [],
      "source": [
        "#0501\n",
        "import numpy as np\n",
        "\n",
        "def MSE(y, t):\n",
        "    return np.sum((y-t)**2)/t.size\n",
        "\n",
        "t = np.array([1,    2, 3,   4])    \n",
        "y1 = np.array([0.5, 1, 1.5, 2])\n",
        "\n",
        "print(\"MSE(t, y1)=\", MSE(t, y1))\n",
        "\n",
        "y2 = np.array([0.5, 1.5, 2.5, 3.5])\n",
        "print(\"MSE(t, y2)=\", MSE(t, y2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#0502\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def MSE(y, t):\n",
        "    return tf.reduce_mean(tf.square(y - t))   # (y - t)**2\n",
        "\n",
        "t = np.array([1,   2, 3,   4])    \n",
        "y1 = np.array([0.5, 1, 1.5, 2])\n",
        "#t = tf.convert_to_tensor(t, dtype=tf.float32)\n",
        "#y1 = tf.convert_to_tensor(y1, dtype=tf.float32)\n",
        "\n",
        "print(\"MSE(t, y1)=\", MSE(t, y1).numpy())\n",
        "\n",
        "y2 = np.array([0.5, 1.5, 2.5, 3.5])\n",
        "print(\"MSE(t, y2)=\", MSE(t, y2).numpy())\n"
      ],
      "metadata": {
        "id": "_WGWATbodWvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0503\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "t = np.array([1,   2, 3,   4])    \n",
        "y1 = np.array([0.5, 1, 1.5, 2])\n",
        "#t = tf.convert_to_tensor(t, dtype=tf.float32)\n",
        "#y1 = tf.convert_to_tensor(y1, dtype=tf.float32)\n",
        "\n",
        "MSE = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "print(\"MSE(t, y1)=\", MSE(t, y1).numpy())\n",
        "\n",
        "y2 = np.array([0.5, 1.5, 2.5, 3.5])\n",
        "print(\"MSE(t, y2)=\", MSE(t, y2).numpy())\n"
      ],
      "metadata": {
        "id": "KUR-VpAtdcYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0601\n",
        "import numpy as np\n",
        "   \n",
        "def MSE(y, t):\n",
        "    return np.sum((y-t)**2)/t.size\n",
        "    \n",
        "x = np.arange(12) # [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]\n",
        "t = np.arange(12)\n",
        "\n",
        "w = 0.5     # 초기값\n",
        "b = 0\n",
        "lr = 0.001  # 0.01, learning rate\n",
        "\n",
        "loss_list = [ ]\n",
        "for epoch in range(200):\n",
        "    y = w*x + b                      # calculate the output   \n",
        "    dW = np.sum((y-t)*x)/(2*x.size)  # gradients\n",
        "    dB = np.sum((y-t))/(2*x.size)\n",
        "    \n",
        "    w = w - lr*dW     # update parameters\n",
        "    b = b - lr*dB\n",
        "\n",
        "    y = w*x + b       # calculate the output\n",
        "    loss = MSE(y, t)\n",
        "    loss_list.append(loss)\n",
        "##    if not epoch%10:\n",
        "##        print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={}\".format(epoch, w, b, loss))\n",
        "\n",
        "print(\"w={:>.4f}. b={:>.4f}, loss={:>.4f}\".format(w, b, loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_list)\n",
        "plt.show() \n"
      ],
      "metadata": {
        "id": "vxWiD70-dfoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0602\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def MSE(y, t):\n",
        "    return np.sum((y-t)**2)/t.size\n",
        "\n",
        "x = np.arange(12) # [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]\n",
        "t = np.arange(12)\n",
        "\n",
        "w = 0.5\n",
        "b = 0\n",
        "lr = 0.001  # 0.01, learning rate\n",
        "loss_list = [ ]\n",
        "\n",
        "train_size = t.size # 12\n",
        "batch_size = 4\n",
        "K = train_size// batch_size # 3\n",
        "\n",
        "for epoch in range(100):\n",
        "    loss = 0\n",
        "    for step in range(K):\n",
        "        mask = np.random.choice(train_size, batch_size)\n",
        "        x_batch = x[mask]\n",
        "        t_batch = t[mask]\n",
        "        \n",
        "        y = w*x_batch + b                               # calculate the output\n",
        "        dW = np.sum((y-t_batch)*x_batch)/(2*batch_size) # gradients\n",
        "        dB = np.sum((y-t_batch))/(2*batch_size)\n",
        "        \n",
        "        w = w - lr*dW   # update parameters\n",
        "        b = b - lr*dB\n",
        "        \n",
        "        y = w*x_batch + b       # calculate the output\n",
        "        loss += MSE(y, t_batch) # calculate MSE\n",
        "    loss /= K  # average loss\n",
        "    loss_list.append(loss)\n",
        "    if not epoch%10:\n",
        "        print(\"epoch={}: w={:>8.4f}. b={:>8.4f}, loss={}\".format(epoch, w, b, loss))\n",
        "\n",
        "print(\"w={:>8.4f}. b={:>8.4f}, loss={}\".format(w, b, loss))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show() \n"
      ],
      "metadata": {
        "id": "Nl3AKpqVdjgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0701\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.Variable(2.0) # tf.Variable(2.0, trainable=True)\n",
        "y = tf.Variable(3.0) # tf.Variable(3.0, trainable=True)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    z = x**2 + y**2\n",
        "dx, dy = tape.gradient(z, [x, y])\n",
        "\n",
        "print('dx=', dx.numpy())\n",
        "print('dy=', dy.numpy())"
      ],
      "metadata": {
        "id": "45hXMUzgdoCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0702\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.constant(2.0)\n",
        "y = tf.constant(3.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    tape.watch(x)\n",
        "    tape.watch(y)\n",
        "    z = x**2 + y**2\n",
        "dx = tape.gradient(z, x)\n",
        "dy = tape.gradient(z, y)\n",
        "\n",
        "print('dx=', dx.numpy())\n",
        "print('dy=', dy.numpy())"
      ],
      "metadata": {
        "id": "Ozy9G6gOdueb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0703\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.Variable(2.0) # tf.Variable(2.0, trainable=True)\n",
        "y = tf.Variable(3.0) # tf.Variable(3.0, trainable=True)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "    tape.watch(x)\n",
        "    tape.watch(y)\n",
        "    z = x**2 + y**2\n",
        "dx, dy = tape.gradient(z, [x, y])\n",
        "\n",
        "print('dx=', dx.numpy())\n",
        "print('dy=', dy.numpy())"
      ],
      "metadata": {
        "id": "zow6OA7Odxar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0704\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape2:\n",
        "    with tf.GradientTape() as tape1:\n",
        "        y = x**3\n",
        "    dy = tape1.gradient(y, x)\n",
        "dy2 = tape2.gradient(dy, x)\n",
        "print('dy=', dy.numpy())\n",
        "print('dy2=', dy2.numpy())\n"
      ],
      "metadata": {
        "id": "NCdlUeJFd1kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0801\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(12)\n",
        "t = np.arange(12)\n",
        "#x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "#t = tf.convert_to_tensor(t, dtype=tf.float32)\n",
        "\n",
        "w = tf.Variable(0.5)\n",
        "b = tf.Variable(0.0)\n",
        "lr = 0.001   # learning rate\n",
        "\n",
        "loss_list = [ ]  # for graph \n",
        "for epoch in range(100):   \n",
        "    with tf.GradientTape() as tape:\n",
        "        y = x*w + b\n",
        "        loss = tf.reduce_mean(tf.square(y - t))\n",
        "    loss_list.append(loss.numpy())\n",
        "\n",
        "    dW, dB = tape.gradient(loss, [w, b])\n",
        "    w.assign_sub(lr * dW) \n",
        "    b.assign_sub(lr * dB)\n",
        "##    if not epoch%10:\n",
        "##        print(\"epoch={}: w={:>.4f}. b={:>.4f}, loss={}\".format(\n",
        "##               epoch, w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "print(\"w={:>.4f}. b={:>.4f}, loss={}\".format(w.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MjLo4Mqjd4Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0802\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(12)\n",
        "t = np.arange(12)\n",
        "\n",
        "w = tf.Variable(0.5)\n",
        "b = tf.Variable(0.0)\n",
        "lr= 0.001   # learning rate, 0.0001\n",
        "\n",
        "train_size = x.size # 12\n",
        "batch_size = 4\n",
        "K = train_size// batch_size\n",
        "\n",
        "loss_list = [ ]\n",
        "for epoch in range(100):\n",
        "    batch_loss = 0.0\n",
        "    for step in range(K):\n",
        "        mask = np.random.choice(train_size, batch_size)\n",
        "        x_batch = x[mask]\n",
        "        t_batch = t[mask]\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            y = w*x_batch + b\n",
        "            loss = tf.reduce_mean(tf.square(y - t_batch))\n",
        "\n",
        "        dW, dB = tape.gradient(loss, [w, b])\n",
        "        w.assign_sub(lr * dW)\n",
        "        b.assign_sub(lr * dB)\n",
        "\n",
        "        batch_loss += loss.numpy() # pre-update loss\n",
        "    batch_loss /= K #  average loss\n",
        "    loss_list.append(batch_loss) \n",
        "\n",
        "##    if not epoch%10:\n",
        "##        print(\"epoch={}: w={:>.4f}. b={:>.4f}, batch_loss={}\".format(\n",
        "##               epoch, w.numpy(), b.numpy(), batch_loss))\n",
        "\n",
        "print(\"w={:>.4f}. b={:>.4f}, batch_loss={}\".format(w.numpy(), b.numpy(), batch_loss))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ejWEjO2sd7Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0901\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "##def MSE(y, t):\n",
        "##    return tf.reduce_mean(tf.square(y - t)) # (y - t)**2\n",
        "MSE = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "train_data = np.array([ # t = 1*x1 + 2*x2 + 3\n",
        "#  x1, x2, t      \n",
        " [ 1,  0,  4],\n",
        " [ 2,  0,  5],\n",
        " [ 3,  0,  6],\n",
        " [ 4,  0,  7],\n",
        " [ 1,  1,  6],\n",
        " [ 2,  1,  7],\n",
        " [ 3,  1,  8],\n",
        " [ 4,  1,  9]], dtype=np.float32)\n",
        "\n",
        "X = train_data[:, :-1]\n",
        "t = train_data[:, -1:]  \n",
        "#X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "#t = tf.convert_to_tensor(t, dtype=tf.float32)\n",
        "print(\"X=\", X)\n",
        "print(\"t=\", t)\n",
        "\n",
        "tf.random.set_seed(1) # 난수열 초기화\n",
        "W = tf.Variable(tf.random.normal(shape=[2, 1]), )\n",
        "b = tf.Variable(tf.random.normal(shape=[1]))\n",
        "##W = tf.Variable([[0.5],[0.5]], dtype=tf.float32)\n",
        "##b = tf.Variable(0.0)\n",
        "print(\"W=\", W.numpy())\n",
        "print(\"b=\", b.numpy())\n",
        "\n",
        "y = tf.matmul(X, W) + b\n",
        "print(\"y=\", y.numpy())\n",
        "\n",
        "loss = MSE(y, t)\n",
        "print(\"MSE(y, t)=\", loss.numpy())\n"
      ],
      "metadata": {
        "id": "TxkDwNpud-Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0902\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "MSE = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "train_data = np.array([ # t = 1*x1 + 2*x2 + 3\n",
        "#  x1, x2, t      \n",
        " [ 1,  0,  4],\n",
        " [ 2,  0,  5],\n",
        " [ 3,  0,  6],\n",
        " [ 4,  0,  7],\n",
        " [ 1,  1,  6],\n",
        " [ 2,  1,  7],\n",
        " [ 3,  1,  8],\n",
        " [ 4,  1,  9]], dtype=np.float32)\n",
        "\n",
        "X = train_data[:, :-1]\n",
        "t = train_data[:, -1:]\n",
        "\n",
        "tf.random.set_seed(1) # 난수열 초기화\n",
        "W = tf.Variable(tf.random.normal(shape=[2, 1]), )\n",
        "b = tf.Variable(tf.random.normal(shape=[1]))\n",
        "lr = 0.01  # learning rate, 0.001\n",
        "\n",
        "loss_list = [ ]\n",
        "for epoch in range(2000):   \n",
        "    with tf.GradientTape() as tape:\n",
        "        y = tf.matmul(X, W) + b\n",
        "        loss = MSE(y, t)\n",
        "    loss_list.append(loss.numpy())\n",
        "\n",
        "    dW, dB = tape.gradient(loss, [W, b])\n",
        "    W.assign_sub(lr * dW)\n",
        "    b.assign_sub(lr * dB)\n",
        "\n",
        "##    if not epoch%100:\n",
        "##        print(\"epoch={}: loss={}\".format(epoch, loss.numpy()))\n",
        "\n",
        "print(\"W={}. b={}, loss={}\".format(W.numpy(), b.numpy(), loss.numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-qNJVvyweCGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0903\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "MSE = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "train_data = np.array([ # t = 1*x1 + 2*x2 + 3\n",
        "#  x1, x2, t      \n",
        " [ 1,  0,  4],\n",
        " [ 2,  0,  5],\n",
        " [ 3,  0,  6],\n",
        " [ 4,  0,  7],\n",
        " [ 1,  1,  6],\n",
        " [ 2,  1,  7],\n",
        " [ 3,  1,  8],\n",
        " [ 4,  1,  9]], dtype=np.float32)\n",
        "\n",
        "X = train_data[:, :-1]\n",
        "t = train_data[:, -1:]\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "W = tf.Variable(tf.random.normal(shape=[2, 1]), )\n",
        "b = tf.Variable(tf.random.normal(shape=[1]))\n",
        "lr = 0.01   # learning rate, 0.001\n",
        "\n",
        "train_size = X.shape[0]\n",
        "batch_size = 4\n",
        "K = train_size// batch_size\n",
        "\n",
        "loss_list = [ ]\n",
        "for epoch in range(1000):\n",
        "    batch_loss = 0.0\n",
        "    for step in range(K):\n",
        "        mask = np.random.choice(train_size, batch_size)\n",
        "        x_batch = X[mask]\n",
        "        t_batch = t[mask]\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            y = tf.matmul(x_batch, W) + b\n",
        "            loss = MSE(y, t_batch)\n",
        "            \n",
        "        batch_loss += loss.numpy()\n",
        "        \n",
        "        dW, dB = tape.gradient(loss, [W, b])\n",
        "        W.assign_sub(lr * dW)\n",
        "        b.assign_sub(lr * dB)\n",
        "        \n",
        "    batch_loss /= K    \n",
        "    loss_list.append(batch_loss) # average loss    \n",
        "##    if not epoch%100:\n",
        "##            print(\"epoch={}, batch_loss={}\".format(epoch, batch_loss))\n",
        "\n",
        "print(\"W={}. b={}, loss={}\".format(W.numpy(), b.numpy(), batch_loss))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T7NRKwBqeEXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1001\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.1) # learning_rate=0.001\n",
        "##opt = tf.keras.optimizers.Adagrad(0.1) \n",
        "##opt = tf.keras.optimizers.Adam(0.1) \n",
        "##opt = tf.keras.optimizers.RMSprop(0.1)\n",
        "\n",
        "loss_list = [ ]\n",
        "for epoch in range(100):\n",
        "        with tf.GradientTape() as tape:\n",
        "                loss = x**2 + y**2               \n",
        "        loss_list.append(loss.numpy())\n",
        "        \n",
        "##        grads = tape.gradient(loss, [x, y])\n",
        "##        grads_list = [g for g in grads]\n",
        "##        grads_and_vars = zip(grads_list, [x, y])\n",
        "        \n",
        "        dx, dy = tape.gradient(loss, [x, y])\n",
        "        grads_and_vars = zip([dx, dy], [x, y])\n",
        "        opt.apply_gradients(grads_and_vars)\n",
        "\n",
        "##        if not epoch%10:\n",
        "##                print(\"epoch={}: loss={}\".format(epoch, loss.numpy()))\n",
        "                \n",
        "print (\"x={:.5f}, y={:.5f}, loss={}\".format(\n",
        "        x.numpy(), y.numpy(), loss.numpy()))\t\n",
        "plt.plot(loss_list)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tgPvBoYOeHLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1002\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.1) # learning_rate=0.001\n",
        "##opt = tf.keras.optimizers.Adagrad(0.1) \n",
        "##opt = tf.keras.optimizers.Adam(0.1) \n",
        "##opt = tf.keras.optimizers.RMSprop(0.1)\n",
        "\n",
        "loss_list = [ ]\n",
        "for epoch in range(100):\n",
        "        loss = lambda : x**2+ y**2   # function\n",
        "        loss_list.append(loss().numpy())\n",
        "        \n",
        "        opt.minimize(loss, var_list=[x, y])\n",
        "\t\n",
        "        if not epoch%10:\n",
        "                print(\"epoch={}: loss={}\".format(epoch, loss().numpy()))\n",
        "                \n",
        "print (\"x={:.5f}, y={:.5f}, loss={}\".format(\n",
        "        x.numpy(), y.numpy(), loss().numpy()))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NLvRaIeLeKc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1003\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "MSE = tf.keras.losses.MeanSquaredError()\n",
        "def mse_loss():\n",
        "    y = tf.matmul(X, W) + b\n",
        "    return MSE(y, t)\n",
        "##    return tf.reduce_mean(tf.square(y - t))\n",
        "\n",
        "train_data = np.array([ # t = 1*x1 + 2*x2 + 3\n",
        "#  x1, x2, t      \n",
        " [ 1,  0,  4],\n",
        " [ 2,  0,  5],\n",
        " [ 3,  0,  6],\n",
        " [ 4,  0,  7],\n",
        " [ 1,  1,  6],\n",
        " [ 2,  1,  7],\n",
        " [ 3,  1,  8],\n",
        " [ 4,  1,  9]], dtype=np.float32)\n",
        "\n",
        "X = train_data[:, :-1]\n",
        "t = train_data[:, -1:]\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "W = tf.Variable(tf.random.normal(shape=[2, 1]), )\n",
        "b = tf.Variable(tf.random.normal(shape=[1]))\n",
        "\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "##opt = tf.keras.optimizers.Adagrad(0.01)\n",
        "##opt = tf.keras.optimizers.Adam(0.01) \n",
        "##opt = tf.keras.optimizers.RMSprop(0.01)\n",
        "     \n",
        "loss_list = [ ]\n",
        "for epoch in range(1000):   \n",
        "    opt.minimize(mse_loss, var_list= [W, b])\n",
        "    \n",
        "    loss = mse_loss().numpy()\n",
        "    loss_list.append(loss)\n",
        "##    if not epoch % 100:\n",
        "##            print (\"epoch={}: loss={:.5f}\".format(epoch, loss))\t\n",
        "                    \n",
        "print (\"W={}, b={}, loss={}\".format(\n",
        "        W.numpy(), b.numpy(), loss))\t\n",
        "plt.plot(loss_list)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ze2M8lOyebXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1004\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "MSE = tf.keras.losses.MeanSquaredError()\n",
        "def mse_loss():\n",
        "    y = tf.matmul(x_batch, W) + b\n",
        "    return MSE(y, t_batch)\n",
        "##    return tf.reduce_mean(tf.square(y - t_batch))\n",
        "\n",
        "train_data = np.array([ # t = 1*x1 + 2*x2 + 3\n",
        "#  x1, x2, t      \n",
        " [ 1,  0,  4],\n",
        " [ 2,  0,  5],\n",
        " [ 3,  0,  6],\n",
        " [ 4,  0,  7],\n",
        " [ 1,  1,  6],\n",
        " [ 2,  1,  7],\n",
        " [ 3,  1,  8],\n",
        " [ 4,  1,  9]], dtype=np.float32)\n",
        "\n",
        "X = train_data[:, :-1]\n",
        "t = train_data[:, -1:]\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "W = tf.Variable(tf.random.normal(shape=[2, 1]), )\n",
        "b = tf.Variable(tf.random.normal(shape=[1]))\n",
        "\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "##opt = tf.keras.optimizers.Adagrad(0.01)\n",
        "##opt = tf.keras.optimizers.Adam(0.01) \n",
        "##opt = tf.keras.optimizers.RMSprop(0.01)\n",
        "\n",
        "train_size = X.shape[0]\n",
        "batch_size = 4\n",
        "K = train_size// batch_size\n",
        "\n",
        "loss_list = [ ]\n",
        "for epoch in range(1000):\n",
        "    batch_loss = 0.0\n",
        "    for  step  in range(K):\n",
        "        mask = np.random.choice(train_size, batch_size)\n",
        "        x_batch = X[mask]\n",
        "        t_batch = t[mask]\n",
        "        \n",
        "        opt.minimize(mse_loss, var_list= [W, b])\n",
        "        loss = mse_loss().numpy()\n",
        "        batch_loss += loss\n",
        "        \n",
        "    batch_loss /= K # average loss\n",
        "    loss_list.append(batch_loss)\n",
        "##    if not epoch % 100:\n",
        "##            print (\"epoch={}: batch_loss={:.5f}\".format(epoch, batch_loss))\t\n",
        "                    \n",
        "print (\"W={}, b={}, loss={}\".format(\n",
        "        W.numpy(), b.numpy(), batch_loss))\t\n",
        "plt.plot(loss_list)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pwsflOgHeebi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1101\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "MSE = tf.keras.losses.MeanSquaredError()\n",
        "def mse_loss():\n",
        "     y = x*w + b\n",
        "     return MSE(y, t) # tf.reduce_mean(tf.square(y - t))\n",
        "\n",
        "EPOCH = 1000\n",
        "train_size = 20\n",
        "\n",
        "# create the train data\n",
        "tf.random.set_seed(1) # np.random.seed(1)\n",
        "x = tf.linspace(0.0, 10.0, num=train_size) #np.linspace(0.0, 10.0, num=20)\n",
        "w_true, b_true = 3, -10  # truth, line parameters\n",
        "t = x*w_true + b_true + tf.random.normal([train_size], mean=0.0, stddev=2.0)\n",
        "\n",
        "# train parameters\n",
        "w = tf.Variable(tf.random.normal([]))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "loss_list = [ ]\n",
        "for epoch in range(EPOCH):   \n",
        "    opt.minimize(mse_loss, var_list= [w, b])\n",
        "     \n",
        "    loss = mse_loss().numpy()\n",
        "    loss_list.append(loss)\n",
        "    if not epoch%100:\n",
        "        print(\"epoch={}: loss={}\".format(epoch, loss))\n",
        "\n",
        "print(\"w={:>.4f}. b={:>.4f}, loss={}\".format(w.numpy(), b.numpy(), loss))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(x, t.numpy())  # train data plot\n",
        "\n",
        "w_pred, b_pred = w.numpy(), b.numpy() # predicted, line parameters\n",
        "t_pred= x*w_pred + b_pred \n",
        "plt.plot(x, t_pred, 'r-')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WqJk07LBehiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1102\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "MSE = tf.keras.losses.MeanSquaredError()\n",
        "def mse_loss():\n",
        "     y = a*x**2 + b*x+c\n",
        "##     y = a*tf.pow(x, 2) + b**x + c\n",
        "     return MSE(y, t) # tf.reduce_mean(tf.square(y - t))\n",
        "\n",
        "EPOCH = 1000\n",
        "train_size = 20\n",
        "\n",
        "# create the train data\n",
        "tf.random.set_seed(1) # np.random.seed(1)\n",
        "x = tf.linspace(-5.0, 5.0, num=train_size)\n",
        "\n",
        "a_true = tf.Variable(3.0)\n",
        "b_true = tf.Variable(2.0)\n",
        "c_true = tf.Variable(1.0)\n",
        "t = a_true*tf.pow(x, 2) + b_true*x+c_true\n",
        "t += tf.random.normal([train_size], mean=0.0, stddev = 2)\n",
        "#t = tf.add(t, np.random.normal(0, 2.0, train_size))\n",
        "\n",
        "a = tf.Variable(tf.random.normal([]))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "c = tf.Variable(tf.random.normal([]))\n",
        "                \n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "##opt = tf.keras.optimizers.RMSprop(0.01)\n",
        "\n",
        "loss_list = [ ]\n",
        "for epoch in range(EPOCH):   \n",
        "    opt.minimize(mse_loss, var_list= [a, b, c])\n",
        "     \n",
        "    loss = mse_loss().numpy()\n",
        "    loss_list.append(loss)\n",
        "\n",
        "    if not epoch%100:\n",
        "        print(\"epoch={}: loss={}\".format(epoch, loss))      \n",
        "\n",
        "print(\"a={:>.4f}. b={:>.4f}, c={:>.4f}, loss={}\".format(\n",
        "       a.numpy(), b.numpy(), c.numpy(),loss))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(x, t.numpy())\n",
        "\n",
        "t_pred = a*tf.pow(x, 2) + b*x + c # parabola curve\n",
        "plt.plot(x, t_pred, 'red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8LhDWPLLema2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1103\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "MSE = tf.keras.losses.MeanSquaredError()\n",
        "def mse_loss():\n",
        "     y = tf.zeros_like(x)\n",
        "     for i in range(W.shape[0]):\n",
        "          y += W[i]*(x**(i+1))\n",
        "     y += b # bias\n",
        "     return MSE(y, t) # tf.reduce_mean(tf.square(y - t))\n",
        "\n",
        "EPOCH = 5000\n",
        "train_size = 20\n",
        "\n",
        "# create the train data\n",
        "tf.random.set_seed(1)\n",
        "x = tf.linspace(-5.0, 5.0, num=train_size)\n",
        "\n",
        "w_true = tf.Variable([1.0, 2.0, 3.0])\n",
        "b_true = tf.Variable(4.0)    \n",
        "t = w_true[2]*x**3 + w_true[1]*x**2 + w_true[0]*x + b_true      \n",
        "t += tf.random.normal([train_size], mean=0.0, stddev = 30)\n",
        "\n",
        "# train variables\n",
        "n = 4 # n-th polynomial curve\n",
        "W = tf.Variable(tf.random.normal([n]))\n",
        "b = tf.Variable(tf.random.normal([])) \n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "##opt = tf.keras.optimizers.RMSprop(0.01)\n",
        "\n",
        "loss_list = [ ]\n",
        "for epoch in range(EPOCH): \n",
        "    opt.minimize(mse_loss, var_list= [W, b])\n",
        "     \n",
        "    loss = mse_loss().numpy()\n",
        "    loss_list.append(loss)\n",
        "\n",
        "    if not epoch%100:\n",
        "        print(\"epoch={}: loss={}\".format(epoch, loss))\n",
        "        \n",
        "print(\"W={}. b={}, loss={}\".format(W.numpy(), b.numpy(),loss))\n",
        "plt.plot(loss_list)\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(x, t.numpy())\n",
        "\n",
        "# polynomial curve\n",
        "t_pred = tf.zeros_like(x)\n",
        "for i in range(W.shape[0]): # n = W.shape[0] \n",
        "     t_pred += W[i]*(x**(i+1))\n",
        "t_pred += b # bias\n",
        "     \n",
        "plt.plot(x, t_pred, 'red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2E3Ke_jCenUL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}